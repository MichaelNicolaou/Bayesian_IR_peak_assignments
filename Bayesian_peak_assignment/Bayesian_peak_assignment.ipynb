{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48b7d9c",
   "metadata": {},
   "source": [
    "# Bayesian peak assignment #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406fa8d",
   "metadata": {},
   "source": [
    "This script is used to perform Bayesian inference to determine what are possible assignments of theoretical DFT vibrational frequency calculations to experimentally observed IR absorption peaks.\n",
    "\n",
    "It requires that an excel database with the DFT-calculated frequencies and an IR spectrum text dump is available, for formatting, see below (where the images of excel spreadsheets are).\n",
    "\n",
    "The complexity of IR spectra make it difficult to associate with theoretical frequency calculations, as we expect that the DFT-calculation will predict a frequency within a range of deviation from the real, experimental peak. While it can be an easier case for molecules such as Vanillin, when a lot of peaks are involved in a smaller space (such as Syringaldehyde), or the spectral peaks are broadened (such as Hydroxybenzaldehyde), it quickly becomes a difficult and uncertain problem.\n",
    "\n",
    "While it's not going to lead to an exact answer, a Sudoku-esque approach can be taken to make some sense out of the spectral pattern, using deduction based on clues such as \"if theoretical frequency \"x\" goes to experimental peak \"x\", then theoretical frequency \"y\" doesn't fit anywhere\". \n",
    "\n",
    "This is the kind of logic that this script and approach are attempting to utilise to present options that make sense in terms of assignment. While a metric is included and called \"Probability\", it essentially indicates how good the fit is, and it is very possible that the chosen answer won't be the \"most likely\", as other clues such as intensity and chemical sense are applied.\n",
    "\n",
    "An assumption applied here is that ALL theoretical frequencies chosen will be represented experimentally, and as such only identifiable and trusted vibrational modes are chosen. While it might sound like bad practise to hold the theoretical frequencies as indicative of what the experimental peaks should be, the opposite cannot be applied, as the experimental peaks will always be more than the theoretical frequencies, due to phenomena that DFT cannot account for (combination bands, Fermi resonance, higher state vibrations beyond the fundamental, phenomena such as hydrogen bonding, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732622a7",
   "metadata": {},
   "source": [
    "Importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a17dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2112f79",
   "metadata": {},
   "source": [
    "Defining functions that will be used later on for the inference:\n",
    "\n",
    "1. Gaussian:\n",
    "Defines the equation for a Gaussian curve. \n",
    "\n",
    "$$\n",
    "      f(\\chi) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}e^{\\frac{({\\chi-\\mu})^{2}}{2 {\\sigma^{2}}}}\n",
    "$$\n",
    "* $\\chi$ refers to a specific point on the x-axis. \n",
    "* $f(\\chi)$ refers to the y-value for the specific point. \n",
    "* $\\mu$ refers to the distribution's mean value.\n",
    "* $\\sigma$ refers to the distribution's standard deviation.\n",
    "\n",
    "\n",
    "\n",
    "2. Bayes:\n",
    "Defines the equation for Bayes' theorem. \n",
    "\n",
    "$$\n",
    "    P(H|E) = \\frac{P(E|H)P(H)}{P(E)}\n",
    "$$\n",
    "* $P(H|E)$ refers to the posterior probability (i.e. the probability that a hypothesis \"H\" holds true when evidence \"E\" is considered).\n",
    "* $P(E|H)$ refers to the likelihood probability (i.e. the probability fo evidence \"E\" being observed assuming that hypothesis \"H\" is true).\n",
    "* $P(H)$ refers to the prior probability (i.e. the current belief probability for hypothesis \"H\" being true. The posterior calculated from one piece of evidence can then be used as the prior probability when presenting the next piece of evidence).\n",
    "* $P(E)$ refers to the marginal probability (i.e. the full probability space for explaining evidence \"E\", in other words, the product of the likelihoods and prior probabilities for all hypotheses with the current piece of evidence:\n",
    "\n",
    "$$\n",
    "    P(E) = P(E|H_{1})P(H_{1}) * P(E|H_{2})P(H_{2}) * P(E|H_{3})P(H_{3}) * ... * P(E|H_{n})P(H_{n})\n",
    "$$\n",
    "\n",
    "\n",
    "3. Bayes_models:\n",
    "Defines the comparative version of Bayes' theorem between two hypotheses.\n",
    "\n",
    "$$\n",
    "    \\frac{P(H_{1}|E)}{P(H_{2}|E)} = \\frac{P(E|H_{1})}{P(E|H_{2})} \\frac{P(H_{1})}{P(H_{2})}\n",
    "$$\n",
    "\n",
    "* This form of Bayes' theorem results in a ratio (Bayes' factor) which indicates the strength of hypothesis 1 over hypothesis 2 based on the presented evidence. Taking the $log_{10}$ of the Bayes' factor can be used to approximate the odds between the two hypotheses (Jeffreys' scale).\n",
    "\n",
    "4. STD_conv:\n",
    "Converts STD from a percentage (which is used in the current work) to a corresponding numerical value based on the investigated experimental peak's value. A \"unique\" STD for each peak only makes sense because we know that errors scale with the peak frequency, therefore a STD as a percentage of the peak makes sense and ensures that all distributions will have a similar \"width\".\n",
    "\n",
    "$$\n",
    "    STD = (\\chi_{Exp} + (\\chi_{Exp} * \\frac{MSD_{(\\%)}}{100})) * \\frac{STD_{(\\%)}}{100}\n",
    "$$\n",
    "\n",
    "5. Mean_conv:\n",
    "Converts MSD from a percentage (which is used in the current work) to a corresponding numerical value and adds onto the experimental peak. As the MSD is conceptualised as the mean deviation from an experimental value (i.e. how higher or lower from the experimental value a theoretical value will predict on average), then it can be used as the expected value in a probability distribution, which is to say, the \"peak\" of the distribution. As such, this function finds that peak value.\n",
    "\n",
    "$$\n",
    "    MSD = \\chi_{Exp} + (\\chi_{Exp} * \\frac{MSD_{(\\%)}}{100}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08f9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian(x, std, mean):\n",
    "    return (1 / (math.sqrt(2 * math.pi * (std**2)))) * (math.exp(- ((x-mean) ** 2) / (2 * (std ** 2))))\n",
    "\n",
    "def Bayes(Prior, Likelihood, Marginal):\n",
    "    Posterior = (Likelihood * Prior) / Marginal\n",
    "    return Posterior\n",
    "\n",
    "def Bayes_models(Likelihood_Hi, Prior_Hi, Likelihood_Hj, Prior_Hj):\n",
    "    Factor = (Likelihood_Hi / Likelihood_Hj) * (Prior_Hi / Prior_Hj)\n",
    "    return Factor\n",
    "\n",
    "def STD_conv(peak, rel_MSD, rel_STD):\n",
    "    STD = (peak + (peak * (rel_MSD / 100))) * (rel_STD / 100)\n",
    "    return STD\n",
    "\n",
    "def Mean_conv(peak, rel_MSD):\n",
    "    Mean = peak + (peak * (rel_MSD / 100))\n",
    "    return Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61beed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.58807788"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STD_conv(1663, -0.18, 1.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1fb4e",
   "metadata": {},
   "source": [
    "This section defines the molecules that will be studied, and the path to the excel database.\n",
    "\n",
    "1. Molecule_list: \n",
    "Insert the molecules as strings (e.g. \"Vanillin\"). These MUST have corresponding sheets in the spreadsheet with the molecule spelled as defined and with the endings \"-DFT\" and \"-IR\" for the DFT frequencies and IR spectrum, respectively. (e.g. Vanillin-DFT and Vanillin-IR).\n",
    "2. path:\n",
    "Insert location of excel spreadsheet database, for python to read locations, \"\\\" slashes must be replaced by either \"/\" or \"\\\\\", or an r must be written before the location string (e.g. 'C:\\\\User\\\\Location\\\\Database.xlsx', 'C:/User/Location/Database.xlsx' or r'C:\\User\\Location\\Database.xlsx').\n",
    "The excel sheets for the DFT frequencies and IR spectrum must be in the form shown below:\n",
    "\n",
    "![alt text](Excel_DFT.png \"DFT frequencies\")\n",
    "\n",
    "* \"Scaled frequency\" refers to the \"Unscaled Frequency\" after it has been multipled by the \"Scaling factor\". \n",
    "* \"Normal mode\" is hand-written by the author, while it's intended to record the important vibrational modes of the vibrations, in essence, the script only cares about something being written there. These indicate the theoretical frequencies that will be extracted by the script for inference.\n",
    "* \"Normal mode (Not computed for peak assignment) is the same as \"Normal mode\", except the script won't extract values if they don't have an entry written in \"Normal mode\", this is intended as a record for modes that might be important but not appear or appear significantly in the IR spectrum. These won't be extracted and used in the script given that the \"Normal mode\" entry is empty.\n",
    "\n",
    "![alt text](Excel_IR.png \"IR frequencies\")\n",
    "\n",
    "* \"Frequency\" and \"Intensity\" can be easily extracted as such from an IR spectrometer.\n",
    "* \"Difference\" refers to the difference between that row's intensity with the previous row's intensity (e.g. for \"C3\": = B3 - B2). This can be just written for one cell and then dragged down to apply the formula for all rows.\n",
    "* \"Sign\" refers to a measure of the sign of the change, formulated as the product of this row's difference with the next row's difference (e.g. for \"D3\": = C3 * C4 * 1000000), the 1000000 factor is just there to make the resulting number not show a lot of decimal places. This tells the script that there is a change in the spectrum (peak or trough) and whether or the change corresponds to a peak (negative value) or trough (positive value). Again, just including this value and dragging the cell will create the entry for all cells.\n",
    "* \"Observation\" allows for manual assignment or dis-assignment of peaks. While the script will automatically detect all peaks and include them (that is, a point which shows a change from a higher to a lower intensity), it can also include noise, which might be wrongly labelled in the assignment. On the other hand, it's possible that the script might miss a peak because it forms a shoulder to a close-by peak instead of a classically defined individual peak. This column allows the user to manually exclude a peak from the script by including the entry \"Noise\" in the row of the included peak, or manually include a peak into the script by including the entry \"Shoulder\" in the row that best represents the shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65eb0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Molecule_list = [\"Vanillin\"]\n",
    "path = 'Vanillin_results.xlsx'\n",
    "\n",
    "for Molecule in Molecule_list:\n",
    "    \n",
    "    df_th = pd.read_excel(path, str(Molecule) + \"-DFT\").fillna(\"\")\n",
    "    df_exp = pd.read_excel(path, str(Molecule) + \"-IR\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c1514",
   "metadata": {},
   "source": [
    "In this section, the user adds the deviation metrics that represent the certainty of the theoretical method. Particularly the Mean Signed Deviation (MSD) and Standard Deviation (STD) as percentages. Fixed value MSD and STD can also be used if needed, but keep in mind that a different option will need to be selected in the script down the line.\n",
    "\n",
    "In this work, the MSD and STD used for PBE0/6-311++G(2d,2p) vibrational frequencies, scaled by a factor of 0.950, were determined through vanillin by identifying and assigning characteristic peaks by eye (In the case of vanillin, this was feasible due to the clean spectrum and well defined peaks that could be easily assigned). See supporting info (S4 for equations, S12 for data and calculation of metrics and S13 for \"universal\" scaling factor optimisation for PBE0/6-311++G(2d,2p).).\n",
    "\n",
    "A \"Threshold\" value will need to be included (recommended: 50). This tells the script how far apart a theoretical frequency can be from an experimental value before it stops being considered. This is a way for the script to avoid forming all possible theoretical-to-experimental assignments possible, as those expand to astronomical amounts. Instead, the script simply takes a theoretical frequency and only forms assignments with experimental peaks within the given \"Threshold\" value in cm$^{-1}$.\n",
    "\n",
    "As this script is intended for batch processing of multiple molecules in sequence, an \"If\" statement is included in the case the user wants to allow for a higher, or lower, threshold for one specific molecule (In our case, Umbelliferone was given a threshold of 70 due to its highly off-set spectrum). To use that, simply specify the molecule in the \"if\" line and the threshold to be used. If more than one molecules need to have unique thresholds, an \"elif\" statement can be added for each new molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97741747",
   "metadata": {},
   "outputs": [],
   "source": [
    "    MSD = -0.12\n",
    "    STD = 1.18\n",
    "\n",
    "    if Molecule == \"Vanillin\":\n",
    "        Threshold = 50\n",
    "    elif Molecule == \"Vanillin\":\n",
    "        Threshold = 50\n",
    "    else:\n",
    "        Threshold = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3786d3",
   "metadata": {},
   "source": [
    "This section is where the script reads the data provided and creates its own variable lists for all theoretical and all experimental peaks given to it. The only things that need to be provided here are the range within the experimental IR spectrum for the script to scan for peaks. Again, some flexibility is provided by allowing different ranges to be included for different molecules (e.g. in this work, Cinnamaldehyde was found to have C-H wagging peaks at less than 700 cm$^{-1}$, but expanding this range for all molecules would have made the inference take much longer).\n",
    "\n",
    "In this part, the user can change the number ranges in the loop to change the range for a molecule (e.g. within the \"If Molecule == \"Vanillin\":\" loop, the values in both \"if df_exp.loc...\" and \"elif str(df_exp.loc...\" lines (680 and 1800) indicate a scan range from 680 to 1800 cm$^{-1}$. Changing those will change the range appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7a8a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing inference for Vanillin.\n"
     ]
    }
   ],
   "source": [
    "    print(\"Performing inference for \" + str(Molecule) + \".\")\n",
    "    \n",
    "    Theoretical_peaks = []\n",
    "    Experimental_peaks = []\n",
    "    \n",
    "    if Molecule == \"Vanillin\":\n",
    "        for mode in range(0, len(df_th.loc[:, \"Mode\"])):\n",
    "            if str(df_th.loc[mode, \"Normal mode\"]) != \"\":\n",
    "                Theoretical_peaks.append(round(df_th.loc[mode, \"Scaled Frequency\"]))\n",
    "        \n",
    "        for wavenumber in range(0, len(df_exp.loc[:, \"Frequency\"])):\n",
    "            if df_exp.loc[wavenumber, \"Sign\"] < 0 and df_exp.loc[wavenumber, \"Difference\"] > 0 and df_exp.loc[wavenumber, \"Frequency\"] > 680 and df_exp.loc[wavenumber, \"Frequency\"] < 1650 and str(df_exp.loc[wavenumber, \"Observation\"]) != \"Noise\":\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))\n",
    "            elif str(df_exp.loc[wavenumber, \"Observation\"]) == \"Shoulder\" and df_exp.loc[wavenumber, \"Frequency\"] > 680 and df_exp.loc[wavenumber, \"Frequency\"] < 1800:\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))\n",
    "                \n",
    "    elif Molecule == \"Vanillin\":\n",
    "        for mode in range(0, len(df_th.loc[:, \"Mode\"])):\n",
    "            if str(df_th.loc[mode, \"Normal mode\"]) != \"\":\n",
    "                Theoretical_peaks.append(round(df_th.loc[mode, \"Scaled Frequency\"]))\n",
    "        \n",
    "        for wavenumber in range(0, len(df_exp.loc[:, \"Frequency\"])):\n",
    "            if df_exp.loc[wavenumber, \"Sign\"] < 0 and df_exp.loc[wavenumber, \"Difference\"] > 0 and df_exp.loc[wavenumber, \"Frequency\"] > 740 and df_exp.loc[wavenumber, \"Frequency\"] < 1650 and str(df_exp.loc[wavenumber, \"Observation\"]) != \"Noise\":\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))\n",
    "            elif str(df_exp.loc[wavenumber, \"Observation\"]) == \"Shoulder\" and df_exp.loc[wavenumber, \"Frequency\"] > 740 and df_exp.loc[wavenumber, \"Frequency\"] < 1800:\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))\n",
    "                \n",
    "    else:\n",
    "        for mode in range(0, len(df_th.loc[:, \"Mode\"])):\n",
    "            if str(df_th.loc[mode, \"Normal mode\"]) != \"\":\n",
    "                Theoretical_peaks.append(round(df_th.loc[mode, \"Scaled Frequency\"]))\n",
    "        \n",
    "        for wavenumber in range(0, len(df_exp.loc[:, \"Frequency\"])):\n",
    "            if df_exp.loc[wavenumber, \"Sign\"] < 0 and df_exp.loc[wavenumber, \"Difference\"] > 0 and df_exp.loc[wavenumber, \"Frequency\"] > 750 and df_exp.loc[wavenumber, \"Frequency\"] < 1650 and str(df_exp.loc[wavenumber, \"Observation\"]) != \"Noise\":\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))\n",
    "            elif str(df_exp.loc[wavenumber, \"Observation\"]) == \"Shoulder\" and df_exp.loc[wavenumber, \"Frequency\"] > 750 and df_exp.loc[wavenumber, \"Frequency\"] < 1800:\n",
    "                Experimental_peaks.append(round(df_exp.loc[wavenumber, \"Frequency\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58016db8",
   "metadata": {},
   "source": [
    "As the algorithm initially iterated through all possible \"hypotheses\", taken as all the possible assignments of theoretical to experimental peaks (which is to say all the permutations of n-experimental peaks and r-theoretical peaks), it was useful to build in a small segment that calculates how many hypotheses are formed with a given set.\n",
    "\n",
    "$$\n",
    "P(n,r) = \\frac{n!}{(n-r)!}\n",
    "$$    \n",
    "\n",
    "This had proven to be useful after realising the silliness of inputting 14 theoretical peaks and 21 experimental peaks in this case, as it can easily lead to massive computational times.\n",
    "\n",
    "This segment simply prints out how many permutations are possible (i.e. would be formed without the threshold limitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339524e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This combination of peak sets will result in 10137091700736000 hypotheses.\n"
     ]
    }
   ],
   "source": [
    "    Hypotheses_number = math.factorial(len(Experimental_peaks)) / (math.factorial(len(Experimental_peaks) - len(Theoretical_peaks)))\n",
    "    print(\"This combination of peak sets will result in \" + str(int(Hypotheses_number)) + \" hypotheses.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216306f8",
   "metadata": {},
   "source": [
    "Obviously, this is a computational road-block if we want to consider all peaks within the region of interest, so, a loop was coded to build the permutations manually (instead of using available packages), while excluding assignments above the threshold as they appear. This has proven to greatly reduce the number of hypotheses, as removing permutations during the process automatically removes all permutations that would be built on top of that.\n",
    " \n",
    "In this case, the 1E16 possible hypotheses are reduced to a much more manageable 2016, simply by filtering claims which would be too unlikely, which still take a bit of time to process, but are certainly doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed0a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10137091700733984 hypotheses have been filtered, 2016 remaining.\n"
     ]
    }
   ],
   "source": [
    "    Theo = list(range(1, len(Theoretical_peaks)+1))\n",
    "    Exp = list(range(1, len(Experimental_peaks)+1))    \n",
    "    \n",
    "    Hypotheses = []\n",
    "    \n",
    "    temp_hypothesis = []\n",
    "    \n",
    "    exec(\"Unassigned_peaks_\" + Molecule + \" = []\")\n",
    "    Theo_to_remove = []\n",
    "    \n",
    "    for i in Theo:\n",
    "        counter = 0\n",
    "        temp_hypothesis_2 = []\n",
    "        if i == 1:\n",
    "            for j in Exp:\n",
    "                if abs(Theoretical_peaks[i-1] - Experimental_peaks[j-1]) < Threshold:\n",
    "                    temp_hypothesis.append([j])     \n",
    "        else:\n",
    "            for h in temp_hypothesis:\n",
    "                for j in Exp:\n",
    "                    temp_h = []\n",
    "                    if abs(Theoretical_peaks[i-1] - Experimental_peaks[j-1]) < Threshold and j not in h:\n",
    "                        temp_h = h[:]\n",
    "                        temp_h.append(j)\n",
    "                        temp_hypothesis_2.append(temp_h)\n",
    "            if temp_hypothesis_2 != []:\n",
    "                temp_hypothesis = temp_hypothesis_2[:]\n",
    "            elif temp_hypothesis_2 == []:\n",
    "                print(\"Could not fit an assignment for theoretical peak: \" + str(Theoretical_peaks[i-1]))\n",
    "                Theo_to_remove.append(i)    \n",
    "                exec(\"Unassigned_peaks_\" + Molecule + \".append(Theoretical_peaks[i-1])\")\n",
    "    \n",
    "    exec(\"[Theoretical_peaks.remove(i) for i in Unassigned_peaks_\" + Molecule + \"]\")\n",
    "    [Theo.remove(i) for i in Theo_to_remove]\n",
    "    \n",
    "    Hypotheses = temp_hypothesis[:]    \n",
    "    \n",
    "    print(str(int(Hypotheses_number - int(len(Hypotheses)))) + \" hypotheses have been filtered, \" + str(len(Hypotheses)) + \" remaining.\")\n",
    "    \n",
    "    Hypotheses_labelled = [] \n",
    "    Evidence_labelled = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c8fe9",
   "metadata": {},
   "source": [
    "This section creates an appropriately sized processing dataframe, which is to say, a table of H rows (equal to the number of Hypotheses) and E columns (equal to the number of Evidence provided).\n",
    "\n",
    "To specify the connection:\n",
    "\n",
    "1. Hypotheses in this case are considered to be all possible complete sets of assignments of theoretical frequencies to experimental peaks, e.g.: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] is an example hypothesis here, which specifies that theoretical frequency number 1 is assigned to Experimental peak number 2 (The first value indicates that it's the experimental value that we assign to the FIRST theoretical frequency, in this case, experimental peak number 2). As there are 2016 possible sets (i.e. unique sets in which all theoretical frequencies are assigned to different experimental peaks within the threshold range), we assume that there are 2016 possible hypotheses that need to be tested.\n",
    "\n",
    "\n",
    "2. Evidence in this case is presented as each theoretical frequency. This sounds a bit counter-intuitive and is easy to mis-interpret. \n",
    "\n",
    "    * As an analogy, consider a problem where we have 5 keys and 3 locks, we know that 3 of those keys will belong to the 3 locks. In this case, if we try a key on lock 1 and find that it opens it, then the probability of key 2 belonging to lock 2 would be affected, as now we know that we have 4 keys and 2 locks. This is analogous to saying that we have 21 detected experimental peaks and 14 theoretical peaks, assuming that all 14 theoretical peaks must be peaks that can be detected experimentally (from DFT-calculation and chemical knowledge). Therefore, assigning as an example theoretical frequency 1 to experimental peak 1 affects the probability of all other peaks. The difference here is that we are 100% sure that key 1 opens lock 1 in this scenario, this wouldn't be the case for our peaks, but instead, we use a probability (a measure of how well the theoretical frequency fits to the experimental peak, see image below). This means that the final inference for each hypothesis will be a product of well all the theoretical frequencies fit to their assigned experimental peaks.\n",
    "    \n",
    "    * Perhaps the most significant issue this method deals with is considering all other peak assignments as well as the one in the current iteration. This means that not just the closeness of a frequency to a peak is taken into consideration, but also how well the rest of the frequencies will fit with other peaks if this assignment is assumed.\n",
    "    \n",
    "    ![alt text](Peak_distribution.jpg \"Peak probability distribution\")\n",
    "    \n",
    "    In this case, a \"probability distribution function\" for Vanillin's experimental 1298 cm$^{-1}$ peak is shown, after the expected value (mean) has been shifted by the MSD. The \"mean\" value represents the most likely frequency, and how \"likely\" other frequencies are to be calculated as fall within the distribution, in this case, a theoretical frequency of 1274 cm${^-1}$ was selected. The image represents a visual representation of this concept. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "087353b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>E3</th>\n",
       "      <th>E4</th>\n",
       "      <th>E5</th>\n",
       "      <th>E6</th>\n",
       "      <th>E7</th>\n",
       "      <th>E8</th>\n",
       "      <th>E9</th>\n",
       "      <th>E10</th>\n",
       "      <th>E11</th>\n",
       "      <th>E12</th>\n",
       "      <th>E13</th>\n",
       "      <th>E14</th>\n",
       "      <th>Theoretical peak assignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hypothesis 1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 15, 19, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2013</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 18, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2014</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 18, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 19, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypothesis 2016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 19, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  E1   E2   E3   E4   E5   E6   E7   E8   E9  E10  E11  E12  \\\n",
       "Hypothesis 1     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 2     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 3     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 4     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 5     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "...              ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "Hypothesis 2012  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 2013  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 2014  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 2015  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Hypothesis 2016  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                 E13  E14                        Theoretical peak assignment  \n",
       "Hypothesis 1     NaN  NaN  [2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...  \n",
       "Hypothesis 2     NaN  NaN  [2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...  \n",
       "Hypothesis 3     NaN  NaN  [2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 2...  \n",
       "Hypothesis 4     NaN  NaN  [2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 1...  \n",
       "Hypothesis 5     NaN  NaN  [2, 3, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 2...  \n",
       "...              ...  ...                                                ...  \n",
       "Hypothesis 2012  NaN  NaN  [4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 15, 19, 2...  \n",
       "Hypothesis 2013  NaN  NaN  [4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 18, 1...  \n",
       "Hypothesis 2014  NaN  NaN  [4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 18, 2...  \n",
       "Hypothesis 2015  NaN  NaN  [4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 19, 1...  \n",
       "Hypothesis 2016  NaN  NaN  [4, 5, 8, 11, 9, 10, 13, 12, 14, 17, 16, 19, 2...  \n",
       "\n",
       "[2016 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    for i in range(1,len(Hypotheses)+1):\n",
    "        Hypotheses_labelled.append(\"Hypothesis \" + str(i))\n",
    "    \n",
    "    for i in range(1,len(Theoretical_peaks)+1):\n",
    "        Evidence_labelled.append(\"E\"+ str(i))\n",
    "    \n",
    "    Probs = pd.DataFrame(columns = Evidence_labelled, index = Hypotheses_labelled)\n",
    "    Probs[\"Theoretical peak assignment\"] = Hypotheses\n",
    "    \n",
    "    Peaks1 = pd.DataFrame({\"Experimental peak label\": Exp})\n",
    "    Peaks2 = pd.DataFrame({\"Experimental peak\": Experimental_peaks})\n",
    "    Peaks3 = pd.DataFrame({\"Theoretical peak label\": Theo})\n",
    "    Peaks4 = pd.DataFrame({\"Theoretical peak\": Theoretical_peaks})\n",
    "    Peaks = pd.concat([Peaks1, Peaks2, Peaks3, Peaks4], axis = 1)\n",
    "\n",
    "    Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84e93e",
   "metadata": {},
   "source": [
    "This section is now the Bayesian processing loop, during which for each hypothesis, Bayes' theorem (as defined earlier) is applied, taking the likelihood as the y-value for a gaussian probability distribution, at an x-value (the theoretical frequency), centered at the assigned experimental peak affected by the MSD with a spread defined by the STD. This only works with the MSD and STD represented as percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c425b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for E in range(0, len(Theoretical_peaks)):\n",
    "        counter = 0\n",
    "        if E == 0:\n",
    "            for H in Hypotheses:\n",
    "                Marginal = []\n",
    "                for i in Hypotheses:   \n",
    "                    Marginal.append(Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[i[E]-1], MSD, STD), Mean_conv(Experimental_peaks[i[E]-1], MSD)) * (1/len(Hypotheses)))\n",
    "                Marginal = sum(Marginal)\n",
    "                P_H_given_E = Bayes((1/len(Hypotheses)), Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[H[E]-1], MSD, STD), Mean_conv(Experimental_peaks[H[E]-1], MSD)), Marginal)\n",
    "                Probs.iloc[counter, E] = P_H_given_E\n",
    "                counter = counter + 1\n",
    "        else:\n",
    "            for H in Hypotheses:\n",
    "                Marginal = []\n",
    "                counter_2 = 0\n",
    "                for i in Hypotheses:   \n",
    "                    Marginal.append(Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[i[E]-1], MSD, STD), Mean_conv(Experimental_peaks[i[E]-1], MSD)) * Probs.iloc[counter_2, E-1])\n",
    "                    counter_2 = counter_2 + 1\n",
    "                Marginal = sum(Marginal)\n",
    "                P_H_given_E = Bayes(Probs.iloc[counter, E-1], Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[H[E]-1], MSD, STD), Mean_conv(Experimental_peaks[H[E]-1], MSD)), Marginal)\n",
    "                Probs.iloc[counter, E] = P_H_given_E\n",
    "                counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf3f3e",
   "metadata": {},
   "source": [
    "An alternative option is provided below for fixed MSD and STD values. Simply unhash and use instead of the previous option in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073e4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for E in range(0, len(Theoretical_peaks)):\n",
    "    #     counter = 0\n",
    "    #     if E == 0:\n",
    "    #         for H in Hypotheses:\n",
    "    #             Marginal = []\n",
    "    #             for i in Hypotheses:   \n",
    "    #                 Marginal.append(Gaussian(Theoretical_peaks[E], STD, MSD + Experimental_peaks[E]) * (1/len(Hypotheses)))\n",
    "    #             Marginal = sum(Marginal)\n",
    "    #             P_H_given_E = Bayes((1/len(Hypotheses)), Gaussian(Theoretical_peaks[E], STD, MSD + Experimental_peaks[E]), Marginal)\n",
    "    #             Probs.iloc[counter, E] = P_H_given_E\n",
    "    #             counter = counter + 1\n",
    "    #     else:\n",
    "    #         for H in Hypotheses:\n",
    "    #             Marginal = []\n",
    "    #             counter_2 = 0\n",
    "    #             for i in Hypotheses:   \n",
    "    #                 Marginal.append(Gaussian(Theoretical_peaks[E], STD, MSD + Experimental_peaks[E]) * Probs.iloc[counter_2, E-1])\n",
    "    #                 counter_2 = counter_2 + 1\n",
    "    #             Marginal = sum(Marginal)\n",
    "    #             P_H_given_E = Bayes(Probs.iloc[counter, E-1], Gaussian(Theoretical_peaks[E], STD, MSD + Experimental_peaks[E]), Marginal)\n",
    "    #             Probs.iloc[counter, E] = P_H_given_E\n",
    "    #             counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ccff4",
   "metadata": {},
   "source": [
    "This section performs the model comparison version of Bayes' theorem for only the 5 highest probability hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76c35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Hypothesis 1 factor is 1.0\n",
      "Lowest Hypothesis 2 factor is 0.8786532020206881\n",
      "Lowest Hypothesis 3 factor is 0.6565819876497873\n",
      "Lowest Hypothesis 4 factor is 0.5769078658375936\n",
      "Lowest Hypothesis 5 factor is 0.2692290991953564\n"
     ]
    }
   ],
   "source": [
    "    Probs = Probs.sort_values(Probs.columns[-2], ascending = False)\n",
    "    \n",
    "    Hypotheses = Probs.iloc[0:5, -1].tolist()\n",
    "    temp_H = []\n",
    "    for Hypothesis in Hypotheses:\n",
    "        exec(\"temp_H.append(\" +str(Hypothesis)+ \")\")\n",
    "    Hypotheses = temp_H\n",
    "    \n",
    "    Probs_models = pd.DataFrame(columns = range(1, len(Hypotheses)+1), index = range(1, len(Hypotheses)+1)).astype(float)\n",
    "    Probs_models_log = pd.DataFrame(columns = range(1, len(Hypotheses)+1), index = range(1, len(Hypotheses)+1)).astype(float)\n",
    "    \n",
    "    #Relative MSD and STD\n",
    "    for E in range(0, len(Theoretical_peaks)):\n",
    "        if E == 0:\n",
    "            for Hi in range(0, len(Hypotheses)):\n",
    "                for Hj in range(0, len(Hypotheses)):\n",
    "                    if Hj == 0:\n",
    "                        Probs_models.iloc[Hi, Hj] = 1\n",
    "                    else:\n",
    "                        Probs_models.iloc[Hi, Hj] = Bayes_models(\n",
    "                            Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[Hypotheses[Hi][E]-1], MSD, STD), Mean_conv(Experimental_peaks[Hypotheses[Hi][E]-1], MSD)),\n",
    "                            (1/len(Hypotheses)),\n",
    "                            Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD, STD), Mean_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD)),\n",
    "                            (1/len(Hypotheses))\n",
    "                        )\n",
    "                        \n",
    "        else:\n",
    "            for Hi in range(0, len(Hypotheses)):\n",
    "                for Hj in range(0, len(Hypotheses)):\n",
    "                    if Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD, STD), Mean_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD)) == 0 or Probs.iloc[Hj, E-1] == 0:\n",
    "                        Probs_models.iloc[Hi, Hj] = 1\n",
    "                    else:\n",
    "                        Probs_models.iloc[Hi, Hj] = Bayes_models(\n",
    "                            Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[Hypotheses[Hi][E]-1], MSD, STD), Mean_conv(Experimental_peaks[Hypotheses[Hi][E]-1], MSD)),\n",
    "                            Probs.iloc[Hi, E-1],\n",
    "                            Gaussian(Theoretical_peaks[E], STD_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD, STD), Mean_conv(Experimental_peaks[Hypotheses[Hj][E]-1], MSD)),\n",
    "                            Probs.iloc[Hj, E-1]\n",
    "                        )\n",
    "    Probs_models_log = Probs_models.applymap(math.log10)\n",
    "    for Hypothesis in range(1, len(Probs_models.index)+1):\n",
    "        print(\"Lowest Hypothesis \" + str(Hypothesis) + \" factor is \" + str(min(Probs_models.iloc[Hypothesis-1, :])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b62b1",
   "metadata": {},
   "source": [
    "Finally, this section saves the results in a new excel spreadsheet.\n",
    "\n",
    "The spreadsheet contains 4 sheets:\n",
    "1. Legend: Shows all considered theoretical frequencies and experimental peaks, their indexed label and their frequency.\n",
    "2. Probabilites: Shows all hypotheses and their probabilities after every round of evidence (order in descending form in terms of probability of final evidence).\n",
    "3. Model comparisons: Shows the Bayesian factors between the five highest probability hypotheses.\n",
    "4. Model comparsions (log): Shows the $log_{10}$ of the Bayesian factor between the five highest probability hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2ebcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with pd.ExcelWriter('IR_assignments_' + Molecule + '.xlsx') as writer:\n",
    "        Peaks.to_excel(writer, sheet_name= \"Legend\")\n",
    "        Probs.to_excel(writer, sheet_name='Probabilities')\n",
    "        Probs_models.to_excel(writer, sheet_name='Model comparisons')\n",
    "        Probs_models_log.to_excel(writer, sheet_name='Model comparisons (log)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c65aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
